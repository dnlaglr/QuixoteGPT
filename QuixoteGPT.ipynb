{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfKIFxAls8NQpSPk96p2sa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JqA6uKeF9KJ"
      },
      "outputs": [],
      "source": [
        "with open('quixote.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOKKAd0qKBIX",
        "outputId": "f45e509e-4a7d-4210-9d9e-a90f15e1a544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 2166999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocabSize = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocabSize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74JrfN_cKbej",
        "outputId": "3a7bad9d-b0ed-429a-8a24-30ac9a3769ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.01246:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = { i:ch for i, ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"Hello World\"))\n",
        "print(decode(encode(\"Hello World\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVaQRfstKyEQ",
        "outputId": "9bd63849-c61d-4f7b-8604-fe7df4be5443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26, 49, 56, 56, 59, 1, 41, 59, 62, 56, 48]\n",
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make our dataset usable to train model, it must be convereted into its enumerated value relative to the sorted list of characters in the story. Torch.tensor is a multi-dimensional matrix of our specified type long which contains the enumerated values."
      ],
      "metadata": {
        "id": "Evq4nPmCMiU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode dataset and store in torch.Tensor\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71TKYCqpLqKj",
        "outputId": "c7baab87-4cdd-43cc-bc02-b5bb23ff952e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2166999]) torch.int64\n",
            "tensor([21, 26, 19, 34, 38, 23, 36,  1, 27,  9,  0,  0, 41, 26, 27, 21, 26,  1,\n",
            "        38, 36, 23, 19, 38, 37,  1, 33, 24,  1, 38, 26, 23,  1, 21, 26, 19, 36,\n",
            "        19, 21, 38, 23, 36,  1, 19, 32, 22,  1, 34, 39, 36, 37, 39, 27, 38, 37,\n",
            "         1, 33, 24,  1, 38, 26, 23,  1, 24, 19, 31, 33, 39, 37,  1, 25, 23, 32,\n",
            "        38, 30, 23, 31, 19, 32,  1, 22, 33, 32,  0, 35, 39, 27, 42, 33, 38, 23,\n",
            "         1, 33, 24,  1, 30, 19,  1, 31, 19, 32, 21, 26, 19,  0,  0,  0, 27, 58,\n",
            "         1, 45,  1, 66, 53, 56, 56, 45, 51, 49,  1, 59, 50,  1, 30, 45,  1, 31,\n",
            "        45, 58, 47, 52, 45,  7,  1, 64, 52, 49,  1, 58, 45, 57, 49,  1, 59, 50,\n",
            "         1, 67, 52, 53, 47, 52,  1, 27,  1, 52, 45, 66, 49,  1, 58, 59,  1, 48,\n",
            "        49, 63, 53, 62, 49,  1, 64, 59,  1, 47, 45, 56, 56,  1, 64, 59,  0, 57,\n",
            "        53, 58, 48,  7,  1, 64, 52, 49, 62, 49,  1, 56, 53, 66, 49, 48,  1, 58,\n",
            "        59, 64,  1, 56, 59, 58, 51,  1, 63, 53, 58, 47, 49,  1, 59, 58, 49,  1,\n",
            "        59, 50,  1, 64, 52, 59, 63, 49,  1, 51, 49, 58, 64, 56, 49, 57, 49, 58,\n",
            "         1, 64, 52, 45, 64,  1, 55, 49, 49, 60,  1, 45,  1, 56, 45, 58, 47, 49,\n",
            "         0, 53, 58,  1, 64, 52, 49,  1, 56, 45, 58, 47, 49,  8, 62, 45, 47, 55,\n",
            "         7,  1, 45, 58,  1, 59, 56, 48,  1, 46, 65, 47, 55, 56, 49, 62,  7,  1,\n",
            "        45,  1, 56, 49, 45, 58,  1, 52, 45, 47, 55,  7,  1, 45, 58, 48,  1, 45,\n",
            "         1, 51, 62, 49, 69, 52, 59, 65, 58, 48,  1, 50, 59, 62,  0, 47, 59, 65,\n",
            "        62, 63, 53, 58, 51,  9,  1, 19, 58,  1, 59, 56, 56, 45,  1, 59, 50,  1,\n",
            "        62, 45, 64, 52, 49, 62,  1, 57, 59, 62, 49,  1, 46, 49, 49, 50,  1, 64,\n",
            "        52, 45, 58,  1, 57, 65, 64, 64, 59, 58,  7,  1, 45,  1, 63, 45, 56, 45,\n",
            "        48,  1, 59, 58,  1, 57, 59, 63, 64,  0, 58, 53, 51, 52, 64, 63,  7,  1,\n",
            "        63, 47, 62, 45, 60, 63,  1, 59, 58,  1, 37, 45, 64, 65, 62, 48, 45, 69,\n",
            "        63,  7,  1, 56, 49, 58, 64, 53, 56, 63,  1, 59, 58,  1, 24, 62, 53, 48,\n",
            "        45, 69, 63,  7,  1, 45, 58, 48,  1, 45,  1, 60, 53, 51, 49, 59, 58,  1,\n",
            "        59, 62,  1, 63, 59,  1, 49, 68, 64, 62, 45,  0, 59, 58,  1, 37, 65, 58,\n",
            "        48, 45, 69, 63,  7,  1, 57, 45, 48, 49,  1, 45, 67, 45, 69,  1, 67, 53,\n",
            "        64, 52,  1, 64, 52, 62, 49, 49,  8, 61, 65, 45, 62, 64, 49, 62, 63,  1,\n",
            "        59, 50,  1, 52, 53, 63,  1, 53, 58, 47, 59, 57, 49,  9,  1, 38, 52, 49,\n",
            "         1, 62, 49, 63, 64,  1, 59, 50,  1, 53, 64,  0, 67, 49, 58, 64,  1, 53,\n",
            "        58,  1, 45,  1, 48, 59, 65, 46, 56, 49, 64,  1, 59, 50,  1, 50, 53, 58,\n",
            "        49,  1, 47, 56, 59, 64, 52,  1, 45, 58, 48,  1, 66, 49, 56, 66, 49, 64,\n",
            "         1, 46, 62, 49, 49, 47, 52, 49, 63,  1, 45, 58, 48,  1, 63, 52, 59, 49,\n",
            "        63,  1, 64, 59,  1, 57, 45, 64, 47, 52,  0, 50, 59, 62,  1, 52, 59, 56,\n",
            "        53, 48, 45, 69, 63,  7,  1, 67, 52, 53, 56, 49,  1, 59, 58,  1, 67, 49,\n",
            "        49, 55,  8, 48, 45, 69, 63,  1, 52, 49,  1, 57, 45, 48, 49,  1, 45,  1,\n",
            "        46, 62, 45, 66, 49,  1, 50, 53, 51, 65, 62, 49,  1, 53, 58,  1, 52, 53,\n",
            "        63,  1, 46, 49, 63, 64,  0, 52, 59, 57, 49, 63, 60, 65, 58,  9,  1, 26,\n",
            "        49,  1, 52, 45, 48,  1, 53, 58,  1, 52, 53, 63,  1, 52, 59, 65, 63, 49,\n",
            "         1, 45,  1, 52, 59, 65, 63, 49, 55, 49, 49, 60, 49, 62,  1, 60, 45, 63,\n",
            "        64,  1, 50, 59, 62, 64, 69,  7,  1, 45,  1, 58, 53, 49, 47, 49,  1, 65,\n",
            "        58, 48, 49, 62,  0, 64, 67, 49, 58, 64, 69,  7,  1, 45, 58, 48,  1, 45,\n",
            "         1, 56, 45, 48,  1, 50, 59, 62,  1, 64, 52, 49,  1, 50, 53, 49, 56, 48,\n",
            "         1, 45, 58, 48,  1, 57, 45, 62, 55, 49, 64,  8, 60, 56, 45, 47, 49,  7,\n",
            "         1, 67, 52, 59,  1, 65, 63, 49, 48,  1, 64, 59,  1, 63, 45, 48, 48, 56,\n",
            "        49,  1, 64, 52, 49,  0, 52, 45, 47, 55,  1, 45, 63,  1, 67, 49, 56, 56,\n",
            "         1, 45, 63,  1, 52, 45, 58, 48, 56, 49,  1, 64, 52, 49,  1, 46, 53, 56,\n",
            "        56,  8, 52, 59, 59, 55,  9,  1, 38, 52, 49,  1, 45, 51, 49,  1, 59, 50,\n",
            "         1, 64, 52, 53, 63,  1, 51, 49, 58, 64, 56, 49, 57, 45, 58,  1, 59, 50,\n",
            "         1, 59, 65, 62, 63,  0, 67, 45, 63,  1, 46, 59, 62, 48, 49, 62, 53, 58,\n",
            "        51,  1, 59, 58,  1, 50, 53, 50, 64, 69, 16,  1, 52, 49,  1, 67, 45, 63,\n",
            "         1, 59, 50,  1, 45,  1, 52, 45, 62, 48, 69,  1, 52, 45, 46, 53, 64,  7,\n",
            "         1, 63, 60, 45, 62, 49,  7,  1, 51, 45, 65, 58, 64,  8, 50, 49, 45, 64,\n",
            "        65, 62, 49, 48,  7,  1, 45,  0, 66, 49, 62, 69,  1, 49, 45, 62, 56, 69,\n",
            "         1, 62, 53, 63, 49, 62,  1, 45, 58, 48,  1, 45,  1, 51, 62, 49, 45, 64,\n",
            "         1, 63, 60, 59, 62, 64, 63, 57, 45, 58])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data)) # 90% of text for training data, rest for validation\n",
        "trainData = data[:n]\n",
        "valData = data[n:]"
      ],
      "metadata": {
        "id": "m27Y8N11MIBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blockSize = 8\n",
        "trainData[:blockSize + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7DqOqFyNoP3",
        "outputId": "d143b1c6-a5ff-45c8-c5fb-83829c00759d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([21, 26, 19, 34, 38, 23, 36,  1, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizing the multiple context layers in one block size. Useful for efficiency but also for getting transformer network used to seeing contexts of different lengths."
      ],
      "metadata": {
        "id": "uhApohq9Ov0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = trainData[:blockSize] # Inputs to transformer\n",
        "y = trainData[1:blockSize + 1] # Target following context inputs\n",
        "\n",
        "for t in range(blockSize):\n",
        "  context = x[:t + 1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzxp9cjTN_Qw",
        "outputId": "73644c6e-de19-4a08-ca39-3542164bdb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([21]), the target is: 26\n",
            "When input is tensor([21, 26]), the target is: 19\n",
            "When input is tensor([21, 26, 19]), the target is: 34\n",
            "When input is tensor([21, 26, 19, 34]), the target is: 38\n",
            "When input is tensor([21, 26, 19, 34, 38]), the target is: 23\n",
            "When input is tensor([21, 26, 19, 34, 38, 23]), the target is: 36\n",
            "When input is tensor([21, 26, 19, 34, 38, 23, 36]), the target is: 1\n",
            "When input is tensor([21, 26, 19, 34, 38, 23, 36,  1]), the target is: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 4 # Independent sequences processed in parallel\n",
        "blockSize = 8 # Maximum context length for predictions\n",
        "\n",
        "# Generate small batch of data of inputs x and targets y\n",
        "def getBatch(split):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  idx = torch.randint(len(data) - blockSize, (batchSize,))\n",
        "  x = torch.stack([data[i:i + blockSize] for i in idx])\n",
        "  y = torch.stack([data[i + 1:i + blockSize + 1] for i in idx])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = getBatch('train')\n",
        "print(xb)\n",
        "print(yb)\n",
        "\n",
        "for b in range(batchSize):\n",
        "  for t in range(blockSize):\n",
        "    context = xb[b, :t + 1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"When input is {context.tolist()}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L9m744yQGRC",
        "outputId": "9da73afd-623f-44c6-d1a4-fa53c0e55572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[49, 48,  7,  1, 67, 52, 59,  1],\n",
            "        [59, 64, 52, 45, 62, 53, 59,  1],\n",
            "        [59, 65, 16,  3,  1, 45, 58, 48],\n",
            "        [49,  1, 45, 64,  1, 59, 58, 47]])\n",
            "tensor([[48,  7,  1, 67, 52, 59,  1, 47],\n",
            "        [64, 52, 45, 62, 53, 59,  1, 50],\n",
            "        [65, 16,  3,  1, 45, 58, 48,  1],\n",
            "        [ 1, 45, 64,  1, 59, 58, 47, 49]])\n",
            "When input is [49], the target is: 48\n",
            "When input is [49, 48], the target is: 7\n",
            "When input is [49, 48, 7], the target is: 1\n",
            "When input is [49, 48, 7, 1], the target is: 67\n",
            "When input is [49, 48, 7, 1, 67], the target is: 52\n",
            "When input is [49, 48, 7, 1, 67, 52], the target is: 59\n",
            "When input is [49, 48, 7, 1, 67, 52, 59], the target is: 1\n",
            "When input is [49, 48, 7, 1, 67, 52, 59, 1], the target is: 47\n",
            "When input is [59], the target is: 64\n",
            "When input is [59, 64], the target is: 52\n",
            "When input is [59, 64, 52], the target is: 45\n",
            "When input is [59, 64, 52, 45], the target is: 62\n",
            "When input is [59, 64, 52, 45, 62], the target is: 53\n",
            "When input is [59, 64, 52, 45, 62, 53], the target is: 59\n",
            "When input is [59, 64, 52, 45, 62, 53, 59], the target is: 1\n",
            "When input is [59, 64, 52, 45, 62, 53, 59, 1], the target is: 50\n",
            "When input is [59], the target is: 65\n",
            "When input is [59, 65], the target is: 16\n",
            "When input is [59, 65, 16], the target is: 3\n",
            "When input is [59, 65, 16, 3], the target is: 1\n",
            "When input is [59, 65, 16, 3, 1], the target is: 45\n",
            "When input is [59, 65, 16, 3, 1, 45], the target is: 58\n",
            "When input is [59, 65, 16, 3, 1, 45, 58], the target is: 48\n",
            "When input is [59, 65, 16, 3, 1, 45, 58, 48], the target is: 1\n",
            "When input is [49], the target is: 1\n",
            "When input is [49, 1], the target is: 45\n",
            "When input is [49, 1, 45], the target is: 64\n",
            "When input is [49, 1, 45, 64], the target is: 1\n",
            "When input is [49, 1, 45, 64, 1], the target is: 59\n",
            "When input is [49, 1, 45, 64, 1, 59], the target is: 58\n",
            "When input is [49, 1, 45, 64, 1, 59, 58], the target is: 47\n",
            "When input is [49, 1, 45, 64, 1, 59, 58, 47], the target is: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Bigram Language Model"
      ],
      "metadata": {
        "id": "XumBKd7yTRBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as Fn\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "  def __init__(self, vocabSize):\n",
        "    super().__init__()\n",
        "\n",
        "    # Have each token read off the logits for next token from a lookup table\n",
        "    self.tokenEmbeddingTable = nn.Embedding(vocabSize, vocabSize)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Idx and targets are both (B, T) tensor of int\n",
        "    logits = self.tokenEmbeddingTable(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "      loss = Fn.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, maxNewTokens):\n",
        "    for _ in range(maxNewTokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :] # Focus on last time step and becomes (B, C)\n",
        "      probs = Fn.softmax(logits, dim=-1)\n",
        "\n",
        "      nextIdx = torch.multinomial(probs, num_samples=1) # (B, 1) because one target for each batch dimension\n",
        "      idx = torch.cat((idx, nextIdx), dim=1) # Create (B, T + 1) by concatinating sample idx to running sequence\n",
        "    return idx\n",
        "\n",
        "m = BigramLM(vocabSize)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits.shape)\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), maxNewTokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slj1jVg5TVps",
        "outputId": "fd772996-6ebf-4873-ef4e-3bc67edfa451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 71])\n",
            "Loss: 4.954416275024414\n",
            "\n",
            "ArB\n",
            "=RfO(lY.M=uqkqVFmFj!y;J)tlyQivNPUMwgYPX,2(=1uoFvrsAmbCnIV.WsY-CKDOGqYl'PX,0??Gjo\"b4(njenw=f6Ex?:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "NlCgz6bGZk5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batchSize = 32\n",
        "for steps in range(20000):\n",
        "  xb, yb = getBatch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHoGaeQoZl3v",
        "outputId": "57c6d6af-a9f8-4f3e-f41e-5e9059ab6dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3388075828552246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), maxNewTokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMjWjRWFa_Fc",
        "outputId": "863188a6-70e7-47b6-a129-674a9e9305d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " lifom VIND le basareoth t h alis fothaiand gono whe gay Shou uciponos owanancldsussthinds ar maito\n",
            "ite,'s bouche witor\n",
            "w an ivere, pyo trre we halisticallleangh anoved d muthenenthetha ailernd th\n",
            "mpl w iomous g nd s pa r and case p\n",
            "m Que aily.\n",
            "ot h,\n",
            "se an, thosatyor mnd t plllindllake me'Yalin o ad of It I CH m nglllear buttoshe t Sano weaiall t ine heredine\n",
            "\n",
            "Qus r noveraty ff pllyed\n",
            "l by, slure, gin hesthed t f by rmera woffe\n",
            "\n",
            "d mpio, findopam led thiz\n",
            "\n",
            "an ind therenler\n",
            "windin plfon to K\n",
            "tht g\n"
          ]
        }
      ]
    }
  ]
}